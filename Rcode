#Get training data
data<-read.csv("C:/Learning/Machine Learning/pml-training.csv")

#Partition the data into testing and training
inTrain<-createDataPartition(y=data$classe,p=0.8,list=FALSE)
testing<-data[-inTrain,]
training<-data[inTrain,]

#Scaling all the variates on both test and train data
training2<-preProcess(training[,-c(1,3,4,7,160)],method=c("center","scale"))
training3<-predict(training2, training)
testing3<-predict(training2, testing)

#Remove non independent and other non useful columns
col_index<-!grepl("max|min|kurtosis|amplitude|skewness|avg|stddev|var", colnames(training3))
training3<-training3[col_index]
testing3<-testing3[col_index]
training3<-training3[,-c(1:7)]
testing3<-testing3[,-c(1:7)]

#Training on Random Forest Algo using 5 fold cross validation
trainControl <- trainControl(method="cv", number=5, verboseIter=FALSE)
rf<-train(classe~.,method="rf", data=training3, trainControl=trainControl)
x1<-predict(rf,testing3[,-53])
confusionMatrix(x1, testing3$classe)

#Training on GBM
gbm<-train(classe~.,method="gbm", data=training3)
x2<-predict(gbm,testing3[,-53])

#Get test set data
testing_new<-read.csv("C:/Learning/Machine Learning/pml-testing.csv")

#Prediciting on the test set after doing basic transformations
testing_new2<-predict(training2, testing_new)
col_index<-!grepl("max|min|kurtosis|amplitude|skewness|avg|stddev|var", colnames(testing_new2))
testing_new3<-testing_new2[col_index]
testing_new3<-testing_new3[,-c(1:7)]
testing_new3<-testing_new3[,-53]
x3<-predict(rf, testing_new3)
x3
